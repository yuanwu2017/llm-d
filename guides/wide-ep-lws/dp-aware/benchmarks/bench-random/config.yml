load:
  type: constant
  stages:
  - rate: 10
    duration: 180
api: 
  type: completion
server:
  type: vllm
  model_name: "deepseek-ai/DeepSeek-R1-0528"
  base_url: http://llm-d-inference-gateway-istio.llm-d-nebius.svc.cluster.local:80
  ignore_eos: true
tokenizer:
  pretrained_model_name_or_path: deepseek-ai/DeepSeek-R1-0528
data:
  type: random
  input_distribution:
    min: 8000           # min length of the synthetic prompts
    max: 8000           # max length of the synthetic prompts
    mean: 8000          # mean length of the synthetic prompts
    std_dev: 0          # standard deviation of the length of the synthetic prompts
  output_distribution:
    min: 1000           # min length of the output to be generated
    max: 1000           # max length of the output to be generated
    mean: 1000          # mean length of the output to be generated
    std_dev: 0          # standard deviation of the length of the output to be generated
report:
  request_lifecycle:
    summary: true
    per_stage: true
    per_request: true