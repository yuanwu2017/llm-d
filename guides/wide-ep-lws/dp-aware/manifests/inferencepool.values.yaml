inferenceExtension:
  replicas: 1
  flags:
    # in vLLM 10.0+, the metric is renamed while upstream GAIE is still using the old name as default.
    # See https://github.com/kubernetes-sigs/gateway-api-inference-extension/pull/1905.
    kv-cache-usage-percentage-metric: "vllm:kv_cache_usage_perc"
    v: 1
  image:
    name: llm-d-inference-scheduler
    hub: ghcr.io/llm-d
    tag: v0.5.0
  extProcPort: 9002
  monitoring:
    interval: "10s"
    # Service account token secret for authentication
    secret:
      name: wide-ep-gateway-sa-metrics-reader-secret
    # Prometheus ServiceMonitor will be created when enabled for EPP metrics collection
    prometheus:
      enabled: true
  pluginsConfigFile: "custom-plugins.yaml"
  pluginsCustomConfig:
    custom-plugins.yaml: |
      # ALWAYS DO PD IN THIS EXAMPLE (THRESHOLD 0)
      # This example uses random routing
      # since it's not yet possible to route to individual DP ranks
      apiVersion: inference.networking.x-k8s.io/v1alpha1
      kind: EndpointPickerConfig
      plugins:
      - type: prefill-header-handler
      - type: prefill-filter
      - type: decode-filter
      - type: prefix-cache-scorer
      - type: active-request-scorer
      - type: queue-scorer
      - type: pd-profile-handler
        parameters:
          threshold: 0
          hashBlockSize: 5
      schedulingProfiles:
      - name: prefill
        plugins:
        - pluginRef: prefill-filter
        - pluginRef: prefix-cache-scorer
          weight: 3
        - pluginRef: active-request-scorer
          weight: 2
        - pluginRef: queue-scorer
          weight: 2
      - name: decode
        plugins:
        - pluginRef: decode-filter
        - pluginRef: active-request-scorer
          weight: 1
        - pluginRef: queue-scorer
          weight: 1

inferencePool:
  targetPorts:
    - number: 8000
    - number: 8001
    - number: 8002
    - number: 8003
    - number: 8004
    - number: 8005
    - number: 8006
    - number: 8007
  modelServers:
    matchLabels:
      llm-d.ai/inference-serving: "true"
      llm-d.ai/guide: "wide-ep-lws"
      llm-d.ai/model: DeepSeek-R1-0528

provider:
  name: istio
  istio:
    destinationRule:
      host: llm-d-infpool-epp
      trafficPolicy:
        tls:
          mode: SIMPLE
          insecureSkipVerify: true
        connectionPool:
          http:
            http1MaxPendingRequests: 256000
            maxRequestsPerConnection: 256000
            http2MaxRequests: 256000
            idleTimeout: "900s"
          tcp:
            maxConnections: 256000
            maxConnectionDuration: "1800s"
            connectTimeout: "900s"
