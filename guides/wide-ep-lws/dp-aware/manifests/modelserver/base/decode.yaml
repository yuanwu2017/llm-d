apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: wide-ep-llm-d-decode
  labels:
    llm-d.ai/inference-serving: "true"
    llm-d.ai/guide: "wide-ep-lws"
    llm-d.ai/accelerator-variant: "gpu"
    llm-d.ai/accelerator-vendor: "nvidia"
    llm-d.ai/model: DeepSeek-R1-0528
    llm-d.ai/role: decode
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 2
    workerTemplate:
      metadata:
        labels:
          llm-d.ai/inference-serving: "true"
          llm-d.ai/guide: "wide-ep-lws"
          llm-d.ai/accelerator-variant: "gpu"
          llm-d.ai/accelerator-vendor: "nvidia"
          llm-d.ai/model: DeepSeek-R1-0528
          llm-d.ai/role: decode
      spec:
        serviceAccountName: deepseek-r1

        volumes:
          - name: dshm
            emptyDir:
              medium: Memory
              sizeLimit: 2Gi  # roughly 32MB per local DP plus scratch space
          - name: hf-cache
            emptyDir: {}
          - name: jit-cache
            emptyDir: {}
        containers:
          - name: vllm
            image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
            securityContext:
              capabilities:
                add:
                  - IPC_LOCK
                  - SYS_RAWIO
              runAsGroup: 0
              runAsUser: 0
            imagePullPolicy: Always
            command:
              - /bin/bash
              - -c
            args:
              - |-
                # Clear /dev/shm on start to prevent running out of space when crashes occur
                # https://github.com/llm-d/llm-d/issues/352
                find /dev/shm -type f -delete

                #################
                # RUN vLLM decode workers
                #################
                START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))

                # --enable-expert-parallel:  Use TPxDP in attention, EP in MoE layers
                # --async-scheduling: Reduce white space between engine steps
                # --enable-dbo:  Dual batch overlap (DBO) overlaps compute with collective communication.

                COMMON_VLLM_ARGS=(
                  --disable-uvicorn-access-log
                  --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL))
                  --data-parallel-address="${LWS_LEADER_ADDRESS}"
                  --data-parallel-rpc-port=5555
                  --enable-expert-parallel
                  --kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}'
                  --all2all-backend deepep_low_latency
                  --enable-dbo
                  --dbo-decode-token-threshold 32
                  --enable-eplb
                  --eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}'
                  --kv-cache-memory-bytes=${KV_CACHE_MEMORY_BYTES-}
                )

                CUDA_VISIBLE_DEVICES="0" vllm serve deepseek-ai/DeepSeek-R1-0528 --host=0.0.0.0 --port=8200 --data-parallel-rank=$((START_RANK + 0)) "${COMMON_VLLM_ARGS[@]}" &
                CUDA_VISIBLE_DEVICES="1" vllm serve deepseek-ai/DeepSeek-R1-0528 --host=0.0.0.0 --port=8201 --data-parallel-rank=$((START_RANK + 1)) "${COMMON_VLLM_ARGS[@]}" &
                CUDA_VISIBLE_DEVICES="2" vllm serve deepseek-ai/DeepSeek-R1-0528 --host=0.0.0.0 --port=8202 --data-parallel-rank=$((START_RANK + 2)) "${COMMON_VLLM_ARGS[@]}" &
                CUDA_VISIBLE_DEVICES="3" vllm serve deepseek-ai/DeepSeek-R1-0528 --host=0.0.0.0 --port=8203 --data-parallel-rank=$((START_RANK + 3)) "${COMMON_VLLM_ARGS[@]}" &
                CUDA_VISIBLE_DEVICES="4" vllm serve deepseek-ai/DeepSeek-R1-0528 --host=0.0.0.0 --port=8204 --data-parallel-rank=$((START_RANK + 4)) "${COMMON_VLLM_ARGS[@]}" &
                CUDA_VISIBLE_DEVICES="5" vllm serve deepseek-ai/DeepSeek-R1-0528 --host=0.0.0.0 --port=8205 --data-parallel-rank=$((START_RANK + 5)) "${COMMON_VLLM_ARGS[@]}" &
                CUDA_VISIBLE_DEVICES="6" vllm serve deepseek-ai/DeepSeek-R1-0528 --host=0.0.0.0 --port=8206 --data-parallel-rank=$((START_RANK + 6)) "${COMMON_VLLM_ARGS[@]}" &
                CUDA_VISIBLE_DEVICES="7" vllm serve deepseek-ai/DeepSeek-R1-0528 --host=0.0.0.0 --port=8207 --data-parallel-rank=$((START_RANK + 7)) "${COMMON_VLLM_ARGS[@]}" &
                wait
              - vllm-parallel-launcher

            env:
              - name: VLLM_MOE_DP_CHUNK_SIZE
                value: "384"  # vLLM default is 256
              - name: DP_SIZE_LOCAL
                value: "8"
              - name: TP_SIZE
                value: "1"
              - name: TRITON_LIBCUDA_PATH
                value: /usr/lib64
              - name: VLLM_SKIP_P2P_CHECK
                value: "1"
              - name: VLLM_USE_DEEP_GEMM
                value: "1"
              - name: NVIDIA_GDRCOPY
                value: enabled
              - name: NVSHMEM_REMOTE_TRANSPORT
                value: ibgda
              - name: NVSHMEM_IB_ENABLE_IBGDA
                value: "true"
              - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                value: eth0
              - name: GLOO_SOCKET_IFNAME
                value: eth0
              - name: NCCL_SOCKET_IFNAME
                value: eth0
              - name: VLLM_LOGGING_LEVEL
                value: INFO
              - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                valueFrom:
                  fieldRef:
                    fieldPath: status.podIP
              - name: CUDA_CACHE_PATH
                value: /var/cache/vllm/cuda
              - name: CCACHE_DIR
                value: /var/cache/vllm/ccache
              - name: VLLM_CACHE_ROOT
                value: /var/cache/vllm/vllm
              - name: FLASHINFER_WORKSPACE_BASE
                value: /var/cache/vllm/flashinfer
              # HuggingFace is likely to be quite a bit larger, give it its own cache
              - name: HF_HUB_CACHE
                value: /var/cache/huggingface

            ports:
              - containerPort: 8200
                name: rank0
                protocol: TCP
              - containerPort: 8201
                name: rank1
                protocol: TCP
              - containerPort: 8202
                name: rank2
                protocol: TCP
              - containerPort: 8203
                name: rank3
                protocol: TCP
              - containerPort: 8204
                name: rank4
                protocol: TCP
              - containerPort: 8205
                name: rank5
                protocol: TCP
              - containerPort: 8206
                name: rank6
                protocol: TCP
              - containerPort: 8207
                name: rank7
                protocol: TCP

            startupProbe:
              httpGet:
                path: /health
                port: rank0
              initialDelaySeconds: 0
              periodSeconds: 1
              timeoutSeconds: 5
              failureThreshold: 2700
            livenessProbe:
              httpGet:
                path: /health
                port: rank0
              periodSeconds: 30
              timeoutSeconds: 5
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /v1/models
                port: rank0
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 3
            resources:
              limits:
                memory: 512Gi
                nvidia.com/gpu: "8"
              requests:
                cpu: 32
                memory: 512Gi
                nvidia.com/gpu: "8"
            volumeMounts:
              - name: dshm
                mountPath: /dev/shm
              - name: hf-cache
                mountPath: /var/cache/huggingface
              - name: jit-cache
                mountPath: /var/cache/vllm

          - name: routing-proxy-rank0
            image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
            imagePullPolicy: Always
            args:
              - --port=8000
              - --vllm-port=8200
              - --connector=nixlv2
              - --zap-log-level=4
              - --secure-proxy=false
              - --enable-prefiller-sampling
            ports:
              - containerPort: 8000
                name: proxy-rank0
                protocol: TCP
            resources: {}
            securityContext:
              allowPrivilegeEscalation: false
              runAsNonRoot: true

          - name: routing-proxy-rank1
            image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
            imagePullPolicy: Always
            args:
              - --port=8001
              - --vllm-port=8201
              - --connector=nixlv2
              - --zap-log-level=4
              - --secure-proxy=false
              - --enable-prefiller-sampling
            ports:
              - containerPort: 8001
                name: proxy-rank1
                protocol: TCP
            resources: {}
            securityContext:
              allowPrivilegeEscalation: false
              runAsNonRoot: true

          - name: routing-proxy-rank2
            image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
            imagePullPolicy: Always
            args:
              - --port=8002
              - --vllm-port=8202
              - --connector=nixlv2
              - --zap-log-level=4
              - --secure-proxy=false
              - --enable-prefiller-sampling
            ports:
              - containerPort: 8002
                name: proxy-rank2
                protocol: TCP
            resources: {}
            securityContext:
              allowPrivilegeEscalation: false
              runAsNonRoot: true

          - name: routing-proxy-rank3
            image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
            imagePullPolicy: Always
            args:
              - --port=8003
              - --vllm-port=8203
              - --connector=nixlv2
              - --zap-log-level=4
              - --secure-proxy=false
              - --enable-prefiller-sampling
            ports:
              - containerPort: 8003
                name: proxy-rank3
                protocol: TCP
            resources: {}
            securityContext:
              allowPrivilegeEscalation: false
              runAsNonRoot: true

          - name: routing-proxy-rank4
            image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
            imagePullPolicy: Always
            args:
              - --port=8004
              - --vllm-port=8204
              - --connector=nixlv2
              - --zap-log-level=4
              - --secure-proxy=false
              - --enable-prefiller-sampling
            ports:
              - containerPort: 8004
                name: proxy-rank4
                protocol: TCP
            resources: {}
            securityContext:
              allowPrivilegeEscalation: false
              runAsNonRoot: true

          - name: routing-proxy-rank5
            image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
            imagePullPolicy: Always
            args:
              - --port=8005
              - --vllm-port=8205
              - --connector=nixlv2
              - --zap-log-level=4
              - --secure-proxy=false
              - --enable-prefiller-sampling
            ports:
              - containerPort: 8005
                name: proxy-rank5
                protocol: TCP
            resources: {}
            securityContext:
              allowPrivilegeEscalation: false
              runAsNonRoot: true

          - name: routing-proxy-rank6
            image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
            imagePullPolicy: Always
            args:
              - --port=8006
              - --vllm-port=8206
              - --connector=nixlv2
              - --zap-log-level=4
              - --secure-proxy=false
              - --enable-prefiller-sampling
            ports:
              - containerPort: 8006
                name: proxy-rank6
                protocol: TCP
            resources: {}
            securityContext:
              allowPrivilegeEscalation: false
              runAsNonRoot: true

          - name: routing-proxy-rank7
            image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
            imagePullPolicy: Always
            args:
              - --port=8007
              - --vllm-port=8207
              - --connector=nixlv2
              - --zap-log-level=4
              - --secure-proxy=false
              - --enable-prefiller-sampling
            ports:
              - containerPort: 8007
                name: proxy-rank7
                protocol: TCP
            resources: {}
            securityContext:
              allowPrivilegeEscalation: false
              runAsNonRoot: true
