apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: prefill
  labels:
    llm-d.ai/inference-serving: "true"
    llm-d.ai/guide: "wide-ep-lws"
    llm-d.ai/accelerator-variant: "gpu"
    llm-d.ai/accelerator-vendor: "nvidia"
    llm-d.ai/model: DeepSeek-R1-0528
    llm-d.ai/role: prefill
spec:
  replicas: 2
  leaderWorkerTemplate:
    size: 1
    workerTemplate:
      metadata:
        labels:
          llm-d.ai/inference-serving: "true"
          llm-d.ai/guide: "wide-ep-lws"
          llm-d.ai/accelerator-variant: "gpu"
          llm-d.ai/accelerator-vendor: "nvidia"
          llm-d.ai/model: DeepSeek-R1-0528
          llm-d.ai/role: prefill
      spec:
        serviceAccountName: deepseek-r1
        volumes:
          - name: dshm
            emptyDir:
              medium: Memory
              sizeLimit: 2Gi  # roughly 32MB per local DP plus scratch space
          - name: hf-cache
            emptyDir: {}
          - name: jit-cache
            emptyDir: {}
        containers:
          - name: vllm
            image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
            securityContext:
              capabilities:
                add:
                  - IPC_LOCK
                  - SYS_RAWIO
              runAsGroup: 0
              runAsUser: 0
            imagePullPolicy: Always
            command:
              - /bin/bash
              - -c
            args:
              - |-
                # Clear /dev/shm on start to prevent running out of space when crashes occur
                # https://github.com/llm-d/llm-d/issues/352
                find /dev/shm -type f -delete

                #################
                # RUN vLLM prefill workers
                #################
                START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))

                # --enable-expert-parallel: Use TPxDP in attention, EP in MoE layers
                # --async-scheduling: Reduce white space between engine steps
                # --enable-dbo: Dual batch overlap (DBO) overlaps compute with collective communication.
                # --enable-eplb:  Expert-parallel load balancing reduces EP load imbalance by replicating heavily-used experts
                #   Performance-memory tradeoff: on DeepSeekV3 eplb uses an extra 2GB per redundant expert per GPU
                #   Divisibility constraint: num_routed_experts (256 for DSv3) + num_redundant_experts must be divisible by the number of GPUs.

                # Common vLLM flag arguments shared across all prefill ranks
                COMMON_VLLM_ARGS=(
                  --max-model-len 65536
                  --disable-uvicorn-access-log
                  --tensor-parallel-size=1
                  --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL))
                  --data-parallel-address="${LWS_LEADER_ADDRESS}"
                  --data-parallel-rpc-port=5555
                  --enable-expert-parallel
                  --kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}'
                  --all2all-backend deepep_high_throughput
                )

                CUDA_VISIBLE_DEVICES="0" vllm serve deepseek-ai/DeepSeek-R1-0528 --host=0.0.0.0 --port=8000 --data-parallel-rank=$((START_RANK + 0)) "${COMMON_VLLM_ARGS[@]}" &
                CUDA_VISIBLE_DEVICES="1" vllm serve deepseek-ai/DeepSeek-R1-0528 --host=0.0.0.0 --port=8001 --data-parallel-rank=$((START_RANK + 1)) "${COMMON_VLLM_ARGS[@]}" &
                CUDA_VISIBLE_DEVICES="2" vllm serve deepseek-ai/DeepSeek-R1-0528 --host=0.0.0.0 --port=8002 --data-parallel-rank=$((START_RANK + 2)) "${COMMON_VLLM_ARGS[@]}" &
                CUDA_VISIBLE_DEVICES="3" vllm serve deepseek-ai/DeepSeek-R1-0528 --host=0.0.0.0 --port=8003 --data-parallel-rank=$((START_RANK + 3)) "${COMMON_VLLM_ARGS[@]}" &
                CUDA_VISIBLE_DEVICES="4" vllm serve deepseek-ai/DeepSeek-R1-0528 --host=0.0.0.0 --port=8004 --data-parallel-rank=$((START_RANK + 4)) "${COMMON_VLLM_ARGS[@]}" &
                CUDA_VISIBLE_DEVICES="5" vllm serve deepseek-ai/DeepSeek-R1-0528 --host=0.0.0.0 --port=8005 --data-parallel-rank=$((START_RANK + 5)) "${COMMON_VLLM_ARGS[@]}" &
                CUDA_VISIBLE_DEVICES="6" vllm serve deepseek-ai/DeepSeek-R1-0528 --host=0.0.0.0 --port=8006 --data-parallel-rank=$((START_RANK + 6)) "${COMMON_VLLM_ARGS[@]}" &
                CUDA_VISIBLE_DEVICES="7" vllm serve deepseek-ai/DeepSeek-R1-0528 --host=0.0.0.0 --port=8007 --data-parallel-rank=$((START_RANK + 7)) "${COMMON_VLLM_ARGS[@]}" &
                wait
              - vllm-parallel-launcher
            env:
              - name: DP_SIZE_LOCAL
                value: "8"
              - name: TP_SIZE
                value: "1"
              - name: TRITON_LIBCUDA_PATH
                value: /usr/lib64
              - name: VLLM_SKIP_P2P_CHECK
                value: "1"
              - name: VLLM_USE_DEEP_GEMM
                value: "1"
              - name: NVIDIA_GDRCOPY
                value: enabled
              - name: NVSHMEM_REMOTE_TRANSPORT
                value: ibgda
              - name: NVSHMEM_IB_ENABLE_IBGDA
                value: "true"
              - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                value: eth0
              - name: GLOO_SOCKET_IFNAME
                value: eth0
              - name: NCCL_SOCKET_IFNAME
                value: eth0
              - name: VLLM_LOGGING_LEVEL
                value: INFO
              - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                valueFrom:
                  fieldRef:
                    fieldPath: status.podIP

              # Use cache directories from the mounted volume under one easy to mount
              # root directory.
              - name: CUDA_CACHE_PATH
                value: /var/cache/vllm/cuda
              - name: CCACHE_DIR
                value: /var/cache/vllm/ccache
              - name: VLLM_CACHE_ROOT
                value: /var/cache/vllm/vllm
              - name: FLASHINFER_WORKSPACE_BASE
                value: /var/cache/vllm/flashinfer
              # HuggingFace is likely to be quite a bit larger, give it its own cache
              - name: HF_HUB_CACHE
                value: /var/cache/huggingface
            ports:
              - containerPort: 8000
                name: rank0
                protocol: TCP
              - containerPort: 8001
                name: rank1
                protocol: TCP
              - containerPort: 8002
                name: rank2
                protocol: TCP
              - containerPort: 8003
                name: rank3
                protocol: TCP
              - containerPort: 8004
                name: rank4
                protocol: TCP
              - containerPort: 8005
                name: rank5
                protocol: TCP
              - containerPort: 8006
                name: rank6
                protocol: TCP
              - containerPort: 8007
                name: rank7
                protocol: TCP
            startupProbe:
              httpGet:
                path: /health
                port: rank0
              initialDelaySeconds: 0
              periodSeconds: 1
              timeoutSeconds: 5
              failureThreshold: 2700
            livenessProbe:
              httpGet:
                path: /health
                port: rank0
              periodSeconds: 30
              timeoutSeconds: 5
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /v1/models
                port: rank0
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 3
            resources:
              limits:
                ephemeral-storage: 1Ti
                memory: 512Gi
                nvidia.com/gpu: "8"
              requests:
                cpu: 32
                ephemeral-storage: 1Ti
                memory: 512Gi
                nvidia.com/gpu: "8"
            volumeMounts:
              - name: dshm
                mountPath: /dev/shm
              - name: hf-cache
                mountPath: /var/cache/huggingface
              - name: jit-cache
                mountPath: /var/cache/vllm
